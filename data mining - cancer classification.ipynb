{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49eacef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Author: Hevra Petekkaya\n",
    "Project: Data Mining Assignment 2 - Data Classification\n",
    "Due Date: 11th of May, 2023\n",
    "\n",
    "'''\n",
    "\n",
    "# Importing needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.stats import pearsonr\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab5d28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(data):\n",
    "    # Calculate the first and third quartiles and the IQR\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Identify potential outliers using the IQR\n",
    "    outliers = [x for x in data if x < q1 - 1.5*iqr or x > q3 + 1.5*iqr]\n",
    "    \n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12a999f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_depth_binning(data, num_bins):\n",
    "    \"\"\"\n",
    "    Divides the data into num_bins equal-frequency bins.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        The data to be binned.\n",
    "    num_bins : int\n",
    "        The number of equal-frequency bins to divide the data into.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    bin_edges : array-like\n",
    "        An array of num_bins+1 elements containing the edges of the bins.\n",
    "\n",
    "    \"\"\"\n",
    "    # Sort the data\n",
    "    sorted_data = np.sort(data)\n",
    "\n",
    "    # Calculate the number of data points per bin\n",
    "    points_per_bin = len(data) // num_bins\n",
    "\n",
    "    # Determine the edges of the bins\n",
    "    bin_edges = [sorted_data[i*points_per_bin] for i in range(num_bins)]\n",
    "    bin_edges.append(sorted_data[-1])\n",
    "\n",
    "    # Return the edges of the bins\n",
    "    return bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7fe39cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 14)\n"
     ]
    }
   ],
   "source": [
    "# Load Excel file into Pandas DataFrame\n",
    "df = pd.read_csv('cng514-cancer-patient-data-assignment-2.csv')\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "dataset = df.to_numpy()\n",
    "\n",
    "# (1000, 14) => 1000 patients with 14 attributes\n",
    "print(dataset.shape)\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f23ae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'High': 365, 'Medium': 332, 'Low': 303})\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# Risk level has three possible values but first we need to identify a way to map it to a binary form.\n",
    "risk_levels = dataset[:,13]\n",
    "\n",
    "# Count the occurrences of each value in the risk level column\n",
    "counts = Counter(risk_levels)\n",
    "\n",
    "# Print the counts\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51f0c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LabelBinarizer object\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "# Define the mapping of risk levels to binary labels\n",
    "risk_level_map = {'High': 0, 'Medium': 1, 'Low': 1}\n",
    "\n",
    "# Convert the risk level data to binary labels\n",
    "risk_levels_binary = [risk_level_map[risk_level] for risk_level in risk_levels]\n",
    "\n",
    "# Fit the LabelBinarizer object to the binary labels and transform the data\n",
    "risk_levels_binary = lb.fit_transform(risk_levels_binary)\n",
    "risk_levels_binary = [sublist[0] for sublist in risk_levels_binary]  # list comprehension to extract values\n",
    "\n",
    "dataset[:, 13] = risk_levels_binary\n",
    "# Print the binary labels\n",
    "# print(risk_levels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc857197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing values in any attributes/features!\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "\n",
    "# check for missing values using isnan() and any() functions\n",
    "missing_values_present = False\n",
    "for i in range(1, dataset.shape[1]):\n",
    "    if np.isnan(dataset[:, i].astype(float)).any():\n",
    "        print(\"Attribute {} has missing values\".format(i))\n",
    "        missing_values_present = Trues\n",
    "        \n",
    "if missing_values_present == False:\n",
    "    print(\"There are no missing values in any attributes/features!\")\n",
    "        \n",
    "# there are no missing values in the attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23e81eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute 1 has noisy values\n",
      "Outliers are [73, 73, 73, 73, 73, 73, 73, 73, 73, 73]\n"
     ]
    }
   ],
   "source": [
    "# Check for noisy data \n",
    "noisy_values_present = False\n",
    "for i in range(1, dataset.shape[1]):\n",
    "    outliers = identify_outliers(dataset[:, i])\n",
    "    if outliers != []:\n",
    "        print(\"Attribute {} has noisy values\".format(i))\n",
    "        print(\"Outliers are {}\".format(outliers))\n",
    "        noisy_values_present = True\n",
    "        \n",
    "if noisy_values_present == False:\n",
    "    print(\"There are no noisy values in any attributes/features!\")\n",
    "\n",
    "# It appears like age has outliers; however, that isn't really the case.\n",
    "# please refer to the report for detailed explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d90aa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'criterion': 'gini', 'max_depth': 4, 'min_samples_split': 2}\n",
      "Best cross-validation score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 , 13],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# create a decision tree classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# perform grid search with stratified cross-validation\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "grid_search = GridSearchCV(dt, param_grid=param_grid, cv=skf, error_score='raise')\n",
    "grid_search.fit(dataset[:, 1:-1], dataset[:, 13].astype(int))\n",
    "\n",
    "# print the best hyperparameters and the corresponding mean cross-validation score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8afb15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom scoring function to calculate specificity and sensitivity\n",
    "def specificity(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "53edccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 1.0\n",
      "Mean specificity: 1.0\n",
      "Mean sensitivity: 1.0\n"
     ]
    }
   ],
   "source": [
    "# now that we did grid search using stratified cross-validation for hyperparameters and \n",
    "# got the best values for hyperparameters, we know which combination of hyperparameters \n",
    "# will give us the best tree. \n",
    "\n",
    "# Now, in order to find the accuracy, specificity, and sensitivity, we can\n",
    "# perform cross validation on the specific combination of hyperparameters that were the most promising. \n",
    "# Please refer to the report to the Results section for more detail. \n",
    "\n",
    "# Separate the features and target variable\n",
    "X = dataset[:, 1:-1]\n",
    "y = dataset[:, 13].astype(int)\n",
    "\n",
    "# Create a decision tree classifier with the best hyperparameters found through grid search\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth=4, min_samples_split=2)\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Calculate the mean accuracy, specificity, and sensitivity using cross-validation\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'specificity': make_scorer(specificity),\n",
    "           'sensitivity': make_scorer(sensitivity)}\n",
    "scores = cross_validate(dt, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "# Print the mean accuracy, specificity, and sensitivity\n",
    "print(\"Mean accuracy:\", np.mean(scores['test_accuracy']))\n",
    "print(\"Mean specificity:\", np.mean(scores['test_specificity']))\n",
    "print(\"Mean sensitivity:\", np.mean(scores['test_sensitivity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eba81767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[365   0]\n",
      " [  0 635]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = cross_val_predict(dt, X, y, cv=10)\n",
    "conf_mat = confusion_matrix(y, y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70f5a59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=4)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=4)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what we did above was to get a realistic approixmation of how well a model can perform given the dataset. \n",
    "# Consequently, now that we know the performance, we do not need a separate data for testing hence we can use the whole dataset \n",
    "# for training. \n",
    "# Please refer to the report to the Results section for more detailed explanation. \n",
    "\n",
    "# Define the best hyperparameters from the grid search\n",
    "best_params = {'criterion': 'gini', 'max_depth': 4, 'min_samples_split': 2}\n",
    "\n",
    "# Create the decision tree classifier with the best hyperparameters\n",
    "dt_final = DecisionTreeClassifier(criterion=best_params['criterion'],\n",
    "                            max_depth=best_params['max_depth'],\n",
    "                            min_samples_split=best_params['min_samples_split'])\n",
    "\n",
    "# Fit the model on the training set that is the whole dataset\n",
    "dt_final.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb66514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
